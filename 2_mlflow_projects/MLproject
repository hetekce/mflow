# Multiple entry points to preprocess, train, evaluate and validate a dataset within docker
# 
name: my_ml_project

# Docker image to use for this project
docker_env:
  image: my_ml_project_image:latest

# Entry points define runnable commands with parameters
entry_points:
  main:
    command: "python src/main.py --data_path {data_path} --model_path {model_path}"
    parameters:
      data_path: {type: str, default: "./data/raw_data.csv"}
      model_path: {type: str, default: "./models/model.pkl"}

  preprocess:
    command: "python src/preprocess.py --input_path {input_path} --output_path {output_path}"
    parameters:
      input_path: {type: str, default: "./data/raw_data.csv"}
      output_path: {type: str, default: "./data/processed_data.csv"}

  train:
    command: "python src/train.py --data_path {data_path} --model_path {model_path} --epochs {epochs}"
    parameters:
      data_path: {type: str, default: "./data/processed_data.csv"}
      model_path: {type: str, default: "./models/model.pkl"}
      epochs: {type: int, default: 10}

  evaluate:
    command: "python src/evaluate.py --model_path {model_path} --test_data_path {test_data_path}"
    parameters:
      model_path: {type: str, default: "./models/model.pkl"}
      test_data_path: {type: str, default: "./data/test_data.csv"}
